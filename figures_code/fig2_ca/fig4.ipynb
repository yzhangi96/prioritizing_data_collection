{
 "cells": [
  {
   "cell_type": "code",
   "id": "82057652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T21:49:07.425087Z",
     "start_time": "2025-06-18T21:49:06.898391Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cvxpy as cp\n",
    "from scipy.io.arff import loadarff\n",
    "from itertools import product, combinations, chain\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_cores = mp.cpu_count()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T21:49:32.487016Z",
     "start_time": "2025-06-18T21:49:07.446414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Read in ACS Income data. Preprocessing the same as in fig5.ipynb\n",
    "X = pd.read_pickle(\"ACSIncome_transf_all.p\")\n",
    "states = pd.DataFrame(X['ST'].value_counts()).reset_index()['ST'][0:5]  # Use the first 5 states\n",
    "\n",
    "def calc_Xbar(X_train, X_train_ST, subsamp, state):\n",
    "    \"\"\"Calculate Ehat[X]\"\"\"\n",
    "    return (np.mean(X_train[X_train_ST == state].drop([\"PINCP\"], axis=1).sample(n=subsamp), axis=0))\n",
    "\n",
    "\n",
    "def estimate_covariance(X_train, X_train_ST, Xbar_US, subsamp, states):\n",
    "    \"\"\"Estimate distributional covariance matrix\"\"\"\n",
    "    Xbar_kp = Parallel(n_jobs=n_cores, verbose=0)(delayed(calc_Xbar)(X_train, X_train_ST, subsamp, t) for t in states)\n",
    "    Xbar_kp_df = pd.DataFrame(Xbar_kp).T\n",
    "    centered_Xbar = Xbar_kp_df.sub(Xbar_US.values, axis=0)\n",
    "    distr_cov = centered_Xbar.cov()\n",
    "    return distr_cov\n"
   ],
   "id": "5714a0d0d65be058",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Functions to calculate optimal samples and weights",
   "id": "2d6507e48871427c"
  },
  {
   "cell_type": "code",
   "id": "b3b98f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T22:45:03.873241Z",
     "start_time": "2025-06-18T22:45:03.837403Z"
    }
   },
   "source": [
    "def calc_penalized_weight(Sigma, N):\n",
    "    \"\"\"\n",
    "    Calculate optimal weights from Eqn 2 of 3.1, Optimal sampling under size constraint\n",
    "    \n",
    "    :param Sigma: Estimated distributional covariance matrix\n",
    "    :param N: Total sample budget\n",
    "    :return: optimal weights\n",
    "    \"\"\"\n",
    "    # Define the variable\n",
    "    w = cp.Variable(Sigma.shape[0])\n",
    "\n",
    "    # Define the objective \n",
    "    objective = cp.Minimize(cp.quad_form(w, Sigma) + (1 / N) * cp.sum(cp.abs(w)) ** 2)\n",
    "\n",
    "    # Define the constraints (sum of weights equals 1)\n",
    "    constraints = [cp.sum(w) == 1]\n",
    "\n",
    "    # Solve convex optimization\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    # Extract the optimal weights\n",
    "    w_optimal = w.value\n",
    "\n",
    "    return w_optimal\n",
    "\n",
    "\n",
    "def run_experiment_US(X, N, states):\n",
    "    \"\"\"\n",
    "    Run experiment per Fig 4\n",
    "    \n",
    "    :param X: ACS Income data\n",
    "    :param N: Total sample budget\n",
    "    :param states: states to run experiment on\n",
    "    :return: dict of results {mse from using individual states, mse from pooled data, mse from optimal sampling, optimal weights}\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Test train split, stratified by state\n",
    "    stratify_col = X['ST'].copy()\n",
    "    X_train, X_test, X_train_ST, _ = train_test_split(X.drop([\"ST\"], axis=1), X['ST'], test_size=20000,\n",
    "                                                      stratify=stratify_col)\n",
    "    \n",
    "    # Estimate distributional covariance matrix\n",
    "    Xbar_US = np.mean(X_test.drop([\"PINCP\"], axis=1), axis=0)\n",
    "    Sigma = estimate_covariance(X_train, X_train_ST, Xbar_US, 10000, states)\n",
    "    \n",
    "    # Calculate optimal weights\n",
    "    opt_weights = calc_penalized_weight(Sigma, N)\n",
    "    abs_opt_weights = np.abs(opt_weights)\n",
    "    opt_n = np.round(abs_opt_weights / sum(abs_opt_weights) * N, 0)\n",
    "\n",
    "    ##### comparisons\n",
    "    sampled_states = [X_train[X_train_ST == s].sample(n=int(n)) for s, n in zip(states, opt_n)]\n",
    "    pooled_N = round(N / len(states), 0)\n",
    "\n",
    "    ## Prediction using pooled data\n",
    "    pooled_states = pd.concat([X_train[X_train_ST == s].sample(n=int(pooled_N)) for s in states])\n",
    "    y_hat_s = RandomForestRegressor().fit(pooled_states.drop([\"PINCP\"], axis=1), pooled_states[\"PINCP\"]).predict(\n",
    "        X_test.drop([\"PINCP\"], axis=1))\n",
    "    mse_pooled = np.mean((X_test[\"PINCP\"] - y_hat_s) ** 2)\n",
    "\n",
    "    ## Prediction using individual state\n",
    "    individual_states = [X_train[X_train_ST == s].sample(n=N) for s in states]\n",
    "    y_hat_s_list = [RandomForestRegressor().fit(samples_s.drop([\"PINCP\"], axis=1), samples_s[\"PINCP\"]).predict(\n",
    "        X_test.drop([\"PINCP\"], axis=1)) for samples_s in individual_states]\n",
    "    mse_individual_list = [np.mean((X_test[\"PINCP\"] - y) ** 2) for y in y_hat_s_list]\n",
    "\n",
    "    ## Prediction using optimal weighting\n",
    "    y_hat_s_list = [RandomForestRegressor().fit(samples_s.drop([\"PINCP\"], axis=1), samples_s[\"PINCP\"]).predict(\n",
    "        X_test.drop([\"PINCP\"], axis=1)) for samples_s in sampled_states]\n",
    "    y_hat_weighted_list = np.sum([w * y_hat_s for w, y_hat_s in zip(opt_weights, y_hat_s_list)], axis=0)\n",
    "    weighted_mse = np.mean((X_test[\"PINCP\"] - y_hat_weighted_list) ** 2)\n",
    "\n",
    "    results = {\n",
    "        'mse_individual_list': mse_individual_list,\n",
    "        'mse_pooled': mse_pooled,\n",
    "        'weighted_mse': weighted_mse,\n",
    "        'opt_weights': opt_weights,\n",
    "    }\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run experiments in parallel and wrangle for plotting.",
   "id": "2f8d6e245d0fb4cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Caution, takes a while to run. \n",
    "#### The results are saved as 'us_opt_samples_agg_all.csv' and 'us_opt_samples_individual_all.csv' respectively\n",
    "# # Samples\n",
    "# n_k = 1000\n",
    "# trials = 100\n",
    "# N_seq = np.arange(50, 1100, 50)\n",
    "# random.seed(1)\n",
    "# \n",
    "# # Run experiments in parallel\n",
    "# results_list = []\n",
    "# for N in N_seq:\n",
    "#     results = Parallel(n_jobs=n_cores, verbose=0)(\n",
    "#         delayed(run_experiment_US)(X, N, states) for _ in range(trials))\n",
    "#     results_list.append(results)\n"
   ],
   "id": "90ba33881dac2b27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# results_df_list = [pd.DataFrame({\"state\": states,\n",
    "#                                  'weights': [np.mean([results_list[N][t].get('opt_weights')[s] for t in range(trials)])\n",
    "#                                              for s in range(len(states))],\n",
    "#                                  \"mse_individual\": [\n",
    "#                                      np.mean([results_list[N][t].get('mse_individual_list')[s] for t in range(trials)])\n",
    "#                                      for s in range(len(states))], }) for N in range(len(N_seq))\n",
    "#                    ]\n",
    "# weighted_mse = [np.mean([results_list[N][t].get('weighted_mse') for t in range(trials)]) for N in range(len(N_seq))]\n",
    "# pooled_mse = [np.mean([results_list[N][t].get('mse_pooled') for t in range(trials)]) for N in range(len(N_seq))]\n",
    "# \n",
    "# combined_data = pd.DataFrame()\n",
    "# \n",
    "# for i, df in enumerate(results_df_list):\n",
    "#     df['N_seq'] = N_seq[i]\n",
    "#     combined_data = pd.concat([combined_data, df], axis=0)\n",
    "# \n",
    "# melted_data = pd.melt(\n",
    "#     combined_data,\n",
    "#     id_vars=['N_seq', 'state'],\n",
    "#     value_vars='mse_individual',\n",
    "#     var_name='Metric',\n",
    "#     value_name='MSE Value'\n",
    "# )\n",
    "# melted_data.to_csv(\"us_opt_samples_individual_all.csv\", index=False)\n",
    "# mse_data = pd.DataFrame({\n",
    "#     'N_seq': N_seq,\n",
    "#     'Weighted MSE': weighted_mse,\n",
    "#     'Pooled MSE': pooled_mse,\n",
    "# })\n",
    "# mse_data.to_csv(\"us_opt_samples_agg_all.csv\", index=False)\n"
   ],
   "id": "c9cac59e814bea65",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
